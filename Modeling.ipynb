{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/train_processed_20200525.csv', index_col='id')\n",
    "test = pd.read_csv('Data/test_processed_20200525.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train['text'], train['target'], random_state = 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(stop_words='english')\n",
    "tfidf_vec.fit_transform(x_train.values.tolist() + x_test.values.tolist())\n",
    "tfidf_train = tfidf_vec.transform(x_train.values.tolist())\n",
    "tfidf_test = tfidf_vec.transform(x_test.values.tolist())\n",
    "\n",
    "# Keep the TfIdf vector to use it with the real testing data afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use accuracy as our KPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      1.00      0.73      1061\n",
      "           1       1.00      0.05      0.10       843\n",
      "\n",
      "    accuracy                           0.58      1904\n",
      "   macro avg       0.79      0.53      0.41      1904\n",
      "weighted avg       0.76      0.58      0.45      1904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=31)\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [4,5],\n",
    "    'min_samples_leaf' : [10,20],\n",
    "    'n_estimators': [100, 500, 1000]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(estimator = rf, param_grid = rf_params,\n",
    "                       scoring = 'accuracy', cv = 3, n_jobs = 1)\n",
    "\n",
    "rf_grid.fit(tfidf_train, y_train)\n",
    "pred = rf_grid.predict(tfidf_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pasin/opt/anaconda3/envs/env01/lib/python3.7/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass scoring=accuracy, n_jobs=3, iid=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.93      0.83      1061\n",
      "           1       0.87      0.63      0.73       843\n",
      "\n",
      "    accuracy                           0.79      1904\n",
      "   macro avg       0.82      0.78      0.78      1904\n",
      "weighted avg       0.81      0.79      0.79      1904\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pasin/opt/anaconda3/envs/env01/lib/python3.7/site-packages/sklearn/model_selection/_search.py:849: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression(random_state=31)\n",
    "\n",
    "lg_params = {\n",
    "    'C': [1.0, 0.1, 0.0001]\n",
    "}\n",
    "\n",
    "lg_grid = GridSearchCV(lg, lg_params, 'accuracy', 3, 1)\n",
    "lg_grid.fit(tfidf_train, y_train)\n",
    "pred = lg_grid.predict(tfidf_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression performs pretty well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Dense, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1024)              15392768  \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 1025      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 17,492,993\n",
      "Trainable params: 17,492,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=tfidf_train.shape[1]))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "es = EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5709 samples, validate on 1904 samples\n",
      "Epoch 1/10\n",
      "5709/5709 [==============================] - 8s 1ms/step - loss: 0.5839 - acc: 0.6929 - val_loss: 0.4643 - val_acc: 0.7915\n",
      "Epoch 2/10\n",
      "5709/5709 [==============================] - 8s 1ms/step - loss: 0.1861 - acc: 0.9345 - val_loss: 0.5527 - val_acc: 0.7736\n",
      "Epoch 3/10\n",
      "5709/5709 [==============================] - 9s 2ms/step - loss: 0.0740 - acc: 0.9739 - val_loss: 0.5841 - val_acc: 0.7826\n",
      "Epoch 4/10\n",
      "5709/5709 [==============================] - 8s 1ms/step - loss: 0.0503 - acc: 0.9792 - val_loss: 0.6479 - val_acc: 0.7805\n",
      "Epoch 5/10\n",
      "5709/5709 [==============================] - 8s 1ms/step - loss: 0.0380 - acc: 0.9821 - val_loss: 0.7089 - val_acc: 0.7768\n",
      "Epoch 6/10\n",
      "5709/5709 [==============================] - 8s 1ms/step - loss: 0.0341 - acc: 0.9846 - val_loss: 0.8590 - val_acc: 0.7684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1499b9050>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tfidf_train, y_train, batch_size = 128, epochs = 10,\n",
    "          validation_data=(tfidf_test, y_test), callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1904/1904 [==============================] - 0s 165us/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict_classes([tfidf_test], batch_size=1024, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.79      1061\n",
      "           1       0.75      0.72      0.73       843\n",
      "\n",
      "    accuracy                           0.77      1904\n",
      "   macro avg       0.77      0.76      0.76      1904\n",
      "weighted avg       0.77      0.77      0.77      1904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN with Padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # size of each word vector\n",
    "max_features = 5000 # number of unique words used (number of rows in an embedding vector)\n",
    "max_len = 100 # maximum number of words in a question to use\n",
    "\n",
    "# Tokenize the sentences\n",
    "tok = Tokenizer(num_words=max_features)\n",
    "tok.fit_on_texts(list(x_train))\n",
    "x_train_token = tok.texts_to_sequences(x_train)\n",
    "x_test_token = tok.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sentences\n",
    "x_train_pad = pad_sequences(x_train_token, maxlen = max_len)\n",
    "x_test_pad = pad_sequences(x_test_token, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Pre-trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1024)              103424    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 317,649\n",
      "Trainable params: 317,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_vector_length, input_length=max_len))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "es = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pasin/opt/anaconda3/envs/env01/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5709 samples, validate on 1904 samples\n",
      "Epoch 1/10\n",
      "5709/5709 [==============================] - 5s 849us/step - loss: 0.6834 - accuracy: 0.5649 - val_loss: 0.6752 - val_accuracy: 0.5572\n",
      "Epoch 2/10\n",
      "5709/5709 [==============================] - 4s 741us/step - loss: 0.6470 - accuracy: 0.5950 - val_loss: 0.6288 - val_accuracy: 0.6502\n",
      "Epoch 3/10\n",
      "5709/5709 [==============================] - 4s 748us/step - loss: 0.5413 - accuracy: 0.7674 - val_loss: 0.4926 - val_accuracy: 0.7815\n",
      "Epoch 4/10\n",
      "5709/5709 [==============================] - 4s 741us/step - loss: 0.3808 - accuracy: 0.8448 - val_loss: 0.4695 - val_accuracy: 0.8025\n",
      "Epoch 5/10\n",
      "5709/5709 [==============================] - 4s 738us/step - loss: 0.3049 - accuracy: 0.8748 - val_loss: 0.4838 - val_accuracy: 0.8020\n",
      "Epoch 6/10\n",
      "5709/5709 [==============================] - 4s 756us/step - loss: 0.2459 - accuracy: 0.9054 - val_loss: 0.5096 - val_accuracy: 0.8036\n",
      "Epoch 7/10\n",
      "5709/5709 [==============================] - 4s 759us/step - loss: 0.2008 - accuracy: 0.9245 - val_loss: 0.5930 - val_accuracy: 0.7967\n",
      "Epoch 8/10\n",
      "5709/5709 [==============================] - 4s 776us/step - loss: 0.1705 - accuracy: 0.9366 - val_loss: 0.6605 - val_accuracy: 0.7694\n",
      "Epoch 9/10\n",
      "5709/5709 [==============================] - 4s 755us/step - loss: 0.1541 - accuracy: 0.9429 - val_loss: 0.6975 - val_accuracy: 0.7768\n",
      "Epoch 10/10\n",
      "5709/5709 [==============================] - 4s 751us/step - loss: 0.1343 - accuracy: 0.9518 - val_loss: 0.8057 - val_accuracy: 0.7789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x141b12b10>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_pad, y_train, batch_size=512, epochs = 10,\n",
    "          validation_data=(x_test_pad, y_test), callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1904/1904 [==============================] - 0s 236us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.80      1061\n",
      "           1       0.75      0.74      0.75       843\n",
      "\n",
      "    accuracy                           0.78      1904\n",
      "   macro avg       0.78      0.78      0.78      1904\n",
      "weighted avg       0.78      0.78      0.78      1904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict_classes([x_test_pad], batch_size=256, verbose = 1)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = 'Models/glove.6B.100d.txt'\n",
    "def get_coef(word, *arr):\n",
    "    return word, np.asarray(arr, dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(glove_path) as path:\n",
    "    embed_index = dict(get_coef(*i.split(' ')) for i in path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pasin/opt/anaconda3/envs/env01/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "# The embed_index comes in the form of a dictionary with a word\n",
    "# as the key and a respective embedding vector as the value\n",
    "\n",
    "# Use values\n",
    "embed_values = np.stack(embed_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_values.shape\n",
    "\n",
    "# 400,000 words, each with 100 in length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13027"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tok.word_index\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = min(max_features, len(word_index)) + 1\n",
    "\n",
    "# Create an embedding matrix with a specific shape\n",
    "# - no. of rows is nb_words, which is number of words, which should not be more than max_features.\n",
    "# - no. of columns is the length of an embedding vector\n",
    "# - the values inside are not necessary, it will be changed later\n",
    "embed_matrix = np.random.normal(embed_values.mean(), embed_values.std(),\n",
    "                                (nb_words, embed_values.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this will work only if max_features < len(word_index)\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embed_vector = embed_index.get(word)\n",
    "    if embed_vector is not None:\n",
    "        embed_matrix[i] = embed_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "e = Embedding(len(embed_matrix), embed_values.shape[1], weights=[embed_matrix], input_length=max_len, trainable=False)\n",
    "model.add(e)\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5709 samples, validate on 1904 samples\n",
      "Epoch 1/10\n",
      "5709/5709 [==============================] - 6s 1ms/step - loss: 0.5754 - accuracy: 0.6991 - val_loss: 0.5036 - val_accuracy: 0.7652\n",
      "Epoch 2/10\n",
      "5709/5709 [==============================] - 5s 932us/step - loss: 0.5069 - accuracy: 0.7586 - val_loss: 0.4908 - val_accuracy: 0.7652\n",
      "Epoch 3/10\n",
      "5709/5709 [==============================] - 5s 932us/step - loss: 0.4883 - accuracy: 0.7740 - val_loss: 0.4897 - val_accuracy: 0.7631\n",
      "Epoch 4/10\n",
      "5709/5709 [==============================] - 5s 937us/step - loss: 0.4791 - accuracy: 0.7821 - val_loss: 0.4797 - val_accuracy: 0.7836\n",
      "Epoch 5/10\n",
      "5709/5709 [==============================] - 5s 936us/step - loss: 0.4746 - accuracy: 0.7819 - val_loss: 0.4795 - val_accuracy: 0.7831\n",
      "Epoch 6/10\n",
      "5709/5709 [==============================] - 5s 939us/step - loss: 0.4642 - accuracy: 0.7868 - val_loss: 0.4704 - val_accuracy: 0.7925\n",
      "Epoch 7/10\n",
      "5709/5709 [==============================] - 5s 938us/step - loss: 0.4570 - accuracy: 0.7938 - val_loss: 0.4757 - val_accuracy: 0.7847\n",
      "Epoch 8/10\n",
      "5709/5709 [==============================] - 5s 937us/step - loss: 0.4477 - accuracy: 0.7975 - val_loss: 0.4754 - val_accuracy: 0.7878\n",
      "Epoch 9/10\n",
      "5709/5709 [==============================] - 5s 948us/step - loss: 0.4446 - accuracy: 0.7977 - val_loss: 0.4677 - val_accuracy: 0.7889\n",
      "Epoch 10/10\n",
      "5709/5709 [==============================] - 5s 946us/step - loss: 0.4332 - accuracy: 0.8071 - val_loss: 0.4665 - val_accuracy: 0.7904\n",
      "1904/1904 [==============================] - 1s 277us/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train_pad, y_train, batch_size = 128, epochs = 10,\n",
    "          validation_data=(x_test_pad, y_test), callbacks=[es])\n",
    "\n",
    "pred = model.predict_classes([x_test_pad], batch_size=256, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82      1061\n",
      "           1       0.82      0.67      0.74       843\n",
      "\n",
      "    accuracy                           0.79      1904\n",
      "   macro avg       0.80      0.78      0.78      1904\n",
      "weighted avg       0.79      0.79      0.79      1904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the last model is the best.<br><br>\n",
    "Use the last one for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # size of each word vector\n",
    "max_features = 5000 # number of unique words used (number of rows in an embedding vector)\n",
    "max_len = 100 # maximum number of words in a question to use\n",
    "\n",
    "# Tokenize the sentences\n",
    "tok = Tokenizer(num_words=max_features)\n",
    "tok.fit_on_texts(list(train['text']))\n",
    "x_token = tok.texts_to_sequences(train['text'])\n",
    "valid_token = tok.texts_to_sequences(test['text'])\n",
    "\n",
    "# Target variable for training\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sentences\n",
    "x_pad = pad_sequences(x_token, maxlen = max_len)\n",
    "valid_pad = pad_sequences(valid_token, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15676"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tok.word_index\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = min(max_features, len(word_index)) + 1\n",
    "\n",
    "# Create an embedding matrix with a specific shape\n",
    "# - no. of rows is nb_words, which is number of words, which should not be more than max_features.\n",
    "# - no. of columns is the length of an embedding vector\n",
    "# - the values inside are not necessary, it will be changed later\n",
    "embed_matrix = np.random.normal(embed_values.mean(), embed_values.std(),\n",
    "                                (nb_words, embed_values.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this will work only if max_features < len(word_index)\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embed_vector = embed_index.get(word)\n",
    "    if embed_vector is not None:\n",
    "        embed_matrix[i] = embed_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "e = Embedding(len(embed_matrix), embed_values.shape[1], weights=[embed_matrix], input_length=max_len, trainable=False)\n",
    "model.add(e)\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7613/7613 [==============================] - 7s 913us/step - loss: 0.5661 - accuracy: 0.7154\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "Epoch 2/10\n",
      "7613/7613 [==============================] - 7s 886us/step - loss: 0.5088 - accuracy: 0.7544\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "Epoch 3/10\n",
      "7613/7613 [==============================] - 7s 886us/step - loss: 0.4910 - accuracy: 0.7725\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "Epoch 4/10\n",
      "7613/7613 [==============================] - 7s 891us/step - loss: 0.4806 - accuracy: 0.7767\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "Epoch 5/10\n",
      "7613/7613 [==============================] - 7s 873us/step - loss: 0.4734 - accuracy: 0.7796\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "Epoch 6/10\n",
      "7613/7613 [==============================] - 7s 888us/step - loss: 0.4634 - accuracy: 0.7883\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "Epoch 7/10\n",
      "7613/7613 [==============================] - 7s 889us/step - loss: 0.4528 - accuracy: 0.7973\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "Epoch 8/10\n",
      "7613/7613 [==============================] - 7s 932us/step - loss: 0.4461 - accuracy: 0.7943\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "Epoch 9/10\n",
      "7613/7613 [==============================] - 7s 897us/step - loss: 0.4433 - accuracy: 0.7972\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "Epoch 10/10\n",
      "7613/7613 [==============================] - 7s 871us/step - loss: 0.4252 - accuracy: 0.8087\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1904/1904 [==============================] - 1s 293us/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_pad, y, batch_size = 128, epochs = 10, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263/3263 [==============================] - 1s 261us/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict_classes([valid_pad], batch_size=256, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('Data/submission.csv')\n",
    "sub.target = pred\n",
    "sub.to_csv('Answer/RNN_20200525.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
